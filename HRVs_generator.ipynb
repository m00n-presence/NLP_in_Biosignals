{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTQk_2uX_3pY"
      },
      "source": [
        "# Import\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNJ56a4rzFAS"
      },
      "source": [
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BJd8TqCuATLz",
        "outputId": "83794512-1502-4621-9a0c-4dbf80f5fde5"
      },
      "outputs": [],
      "source": [
        "!pip install wfdb tqdm hrv-analysis\n",
        "!pip install -Iv neurokit2==0.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPCXQRlt_0wN"
      },
      "outputs": [],
      "source": [
        "import neurokit2 as nk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import hrvanalysis as hrvana\n",
        "import wfdb\n",
        "import keras\n",
        "import scipy\n",
        "import os\n",
        "import math\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from wfdb import plot\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHSq1eqGTruL"
      },
      "source": [
        "Mount GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcALmSBpE4U8",
        "outputId": "512e3b30-b305-49ed-ca79-5904d840ee81"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywTXL6r_zP0_"
      },
      "source": [
        "List of filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNtnNsMMxlkW"
      },
      "outputs": [],
      "source": [
        "datafolder = \"\" # полный путь до папки с записями\n",
        "files = os.listdir(datafolder)\n",
        "filenames = list(set([datafolder + f.split('.')[0] for f in files]))\n",
        "filenames.sort()\n",
        "# исключение проблемных записей\n",
        "filenames.remove(datafolder + '30')\n",
        "filenames.remove(datafolder + '45')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJjxS7UpB2j4"
      },
      "source": [
        "# Slice record into 5 min length samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjexA-Bwxcp2"
      },
      "source": [
        "Slicing funcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61C3nG2XB1RU"
      },
      "outputs": [],
      "source": [
        "def slice_record_to_5min_intervals(filename, intervals_max_count=180):\n",
        "    \"\"\" Читает запись и делит её на последовательные пятиминутные отрезки,\n",
        "        их максимальное количество = intervals_max_count\n",
        "        Args:\n",
        "            filename: string, полный путь до записи без расширения\n",
        "            intervals_max_count: int, максимальное количество интервалов, \n",
        "                на которые делить запись. Если она короче - вернет меньше интервалов\n",
        "        Returns:\n",
        "            list of lists: внутренние списки - списки сэмплов, входящих в интервалы\n",
        "    \"\"\"\n",
        "    print(\"Now slicing record with path : \" + filename)\n",
        "    annotation = wfdb.rdann(filename,\"atr\")\n",
        "    fs = annotation.fs\n",
        "    sample = annotation.sample\n",
        "    record = wfdb.rdrecord(filename)    \n",
        "    signal = record.__dict__[\"p_signal\"][sample[0]:sample[-1], 0]\n",
        "    \n",
        "    # return slice_signal_to_5min_intervals(signal, fs)\n",
        "    return slice_intersected_5min_intervals(signal, fs) # для временных интервалов с пересечениями"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbzHpjfdFnYA"
      },
      "outputs": [],
      "source": [
        "def slice_signal_to_5min_intervals(signal, sampling_freq, intervals_max_count = 180):\n",
        "    \"\"\" Делит сигнал на последовательные пятиминутные отрезки,\n",
        "        их максимальное количество = intervals_max_count\n",
        "        Args: \n",
        "            signal: list-like, сигнал для деления\n",
        "            sampling_freq: int, частота дискретизации(Гц)\n",
        "            intervals_max_count: int,  максимальное количество интервалов, \n",
        "                на которые делить запись. Если она короче - вернет меньше интервалов\n",
        "        Returns:\n",
        "            list of lists: внутренние списки - списки сэмплов, входящих в интервалы\n",
        "            максимальная длина внешнего списка = intervals_max_count\n",
        "    \"\"\"\n",
        "    interval_length = 5 * 60 * sampling_freq\n",
        "    signal_slices = []\n",
        "    \n",
        "    for i in range(0, len(signal), interval_length):\n",
        "        if (len(signal_slices) >= intervals_max_count):\n",
        "            break\n",
        "        signal_slices.append(signal[i:i + interval_length]) \n",
        "\n",
        "    return signal_slices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7fiz3PUFoMm"
      },
      "outputs": [],
      "source": [
        "# изначальные параметры intervals_max_count=180 (15 часов) и shift_minutes=1\n",
        "def slice_intersected_5min_intervals(signal, sampling_freq, intervals_max_count = 250, shift_minutes = 2):\n",
        "    \"\"\" Делит сигнал на пересекающиеся 5-минутные отрезки \n",
        "        Args: \n",
        "            signal: list-like, сигнал\n",
        "            sampling_freq: int, частота дискретизации(Гц)\n",
        "            intervals_max_count: int, максимальное количество интервалов на возвращение,\n",
        "                может вернуть меньше указанного значения\n",
        "            shift_minutes: int > 0, шаг, примеры:\n",
        "                если 1, то возвратит интервалы 0-5, 1-6, 2-7 и т.д.\n",
        "                если 2, то интервалы 0-5, 2-7, 4-9 и т.д.\n",
        "        Returns:\n",
        "            list of lists - внутренние списки - сэмплы на 5 минут,\n",
        "            максимальное len() внешнего списка = intervals_max_count\n",
        "    \"\"\"\n",
        "    interval_length = 5 * 60 * sampling_freq\n",
        "    shift = shift_minutes * 60 * sampling_freq\n",
        "    signal_slices = []\n",
        "    \n",
        "    for i in range(0, len(signal), shift):\n",
        "        if (len(signal_slices) >= intervals_max_count):\n",
        "            break\n",
        "        current_slice = signal[i:i + interval_length]\n",
        "        signal_slices.append(current_slice)\n",
        "\n",
        "    return signal_slices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEhFNgBRCag7"
      },
      "source": [
        "# Prepare HRV arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LnkCqdhSCKZ"
      },
      "source": [
        "Get peaks and HRV array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9wAu5MLCdmd"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/6518811/interpolate-nan-values-in-a-numpy-array\n",
        "def interpolate_nans(original):\n",
        "    \"\"\" Линейно интерполирует NaN и бесконечные значения в коллекции\n",
        "        Args:\n",
        "            original: array-like, коллекция, в кот-ой нужно интерполировать значения\n",
        "        Returns:\n",
        "            тот же объект, но все NaN и бесконечные значения заменены на \n",
        "            линейно интерполированные значения\n",
        "    \"\"\"\n",
        "    nans, x = ~np.isfinite(original), lambda z: z.nonzero()[0]\n",
        "    original[nans]= np.interp(x(nans), x(~nans), original[~nans])\n",
        "    \n",
        "    return original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL6HHBgmQiSZ"
      },
      "outputs": [],
      "source": [
        "def wave_to_peaks(signal, fs):\n",
        "    \"\"\" Декоратор (Wrapper function) neurokit's 'ecg_findpeaks' и 'ecg_clean'.\n",
        "        Очищает сигнал и возвращает сэмплы, на которых найдены R-пики,\n",
        "        как в документации neurokit по функциям.\n",
        "        Args: \n",
        "            signal: list-like, сигнал\n",
        "            fs: int, частота дискретизации(Гц)\n",
        "        Returns:\n",
        "            dict, доступ к пикам по ключу 'ECG_R_Peaks'\n",
        "    \"\"\"\n",
        "    signal = hrvana.interpolate_nan_values(rr_intervals=signal,interpolation_method=\"linear\")\n",
        "    cleaned = nk.ecg_clean(signal, sampling_rate = fs)\n",
        "    peaks = nk.ecg_findpeaks(cleaned, sampling_rate=fs, show=False)['ECG_R_Peaks']\n",
        "    \n",
        "    return peaks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSDMJEDXYmbB"
      },
      "outputs": [],
      "source": [
        "EXCLUDED_COLUMNS = ['HRV_ULF', 'HRV_VLF', 'HRV_CSI_Modified', 'HRV_S']\n",
        "SAMPLING_FREQ = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgm6N_lNFRN_"
      },
      "outputs": [],
      "source": [
        "def rpeaks_to_hrv(rri, fs):\n",
        "    \"\"\" Возвращает HRV значения из R-пиков\n",
        "        Args:\n",
        "            rpeaks: dict, доступ к пикам по ключу 'ECG_R_Peaks'\n",
        "            fs: int, частота дискретизации(Гц)\n",
        "        Returns:\n",
        "            1-мерный массив показателей, если среди них есть \n",
        "            неопределенные значения (NaN и бесконечность), заменяет их на 0\n",
        "    \"\"\"\n",
        "    clean_rri = rri*(1000/fs)\n",
        "    clean_rri = hrvana.remove_ectopic_beats(rr_intervals=clean_rri, method=\"malik\")\n",
        "    clean_rri = hrvana.interpolate_nan_values(rr_intervals=clean_rri,interpolation_method=\"linear\")\n",
        "\n",
        "    clean_rri = np.array(clean_rri)\n",
        "    clean_rri = clean_rri[~np.isnan(clean_rri)]\n",
        "    \n",
        "    peaks_unec = np.zeros(len(clean_rri)+1)\n",
        "    cv = 0\n",
        "\n",
        "    for count, value in enumerate(clean_rri):\n",
        "        cv += value\n",
        "        peaks_unec[count+1] = cv\n",
        "\n",
        "    peaks_unec *= (128.0/1000.0)   \n",
        "    rpeaks = {'ECG_R_Peaks':peaks_unec}\n",
        "    hrvdat = nk.hrv(rpeaks, sampling_rate=128, show=False)\n",
        "    hrvdat = hrvdat.drop(columns = EXCLUDED_COLUMNS)\n",
        "    indices = np.ravel(hrvdat.to_numpy())\n",
        "    \n",
        "    return indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69P8c3rtSJPV"
      },
      "source": [
        "Normalize the HRV values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-ZVMGnQCjeG"
      },
      "outputs": [],
      "source": [
        "def flatten_1stlevel(list_of_lists_of_lists):\n",
        "    \"\"\" >>> flatten_1stlevel([[[1, 2], [3, 4]],[[5, 6], [7, 8]],[[9, 0]]])\n",
        "        [[1, 2], [3, 4], [5, 6], [7, 8], [9, 0]]\n",
        "        >>> flatten_1stlevel([[1, 2], [[3,4], [[5, 6], 7]], []])\n",
        "        [1, 2, [3, 4], [[5, 6], 7]]\n",
        "        >>> flatten_1stlevel([[1, 2], [3, 4]])\n",
        "        [1, 2, 3, 4]\n",
        "        >>> flatten_1stlevel([1, 2, 3])\n",
        "        [1, 2, 3]\n",
        "    \"\"\"\n",
        "    flat_1stlevel = []\n",
        "    \n",
        "    for list_of_lists in list_of_lists_of_lists:\n",
        "        try:\n",
        "            flat_1stlevel.extend(list_of_lists)\n",
        "        except TypeError:\n",
        "            flat_1stlevel.append(list_of_lists)\n",
        "\n",
        "    return flat_1stlevel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JSLJJkzFOI5"
      },
      "outputs": [],
      "source": [
        "def standardize_hrv_values(hrv_arrays, return_scaler=False):\n",
        "    \"\"\" Нормализует значения показателей с помощью MinMaxScaler\n",
        "        Args:\n",
        "            hrv_arrays: list of arrays, список массивов показателей по пациенту, структура:\n",
        "            [[[1, 2], [3, 4]],[[5, 6], [7, 8]],[[9, 0], [10, 11]]]\n",
        "            return_scaler: bool, true - возвращает обученный MinMaxScaler\n",
        "        Returns:\n",
        "            (всегда) list of numpy arrays - стандартизированных значений,\n",
        "            if return_scaler - также возвратит MinMaxScaler\n",
        "\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    person_hrvs_count = len(hrv_arrays) # количество пациентов\n",
        "    all_hrvs = flatten_1stlevel(hrv_arrays)\n",
        "    scaler = scaler.fit(all_hrvs)\n",
        "    std_hrvs = scaler.transform(all_hrvs)\n",
        "    \n",
        "    if not return_scaler:\n",
        "        return np.split(std_hrvs, person_hrvs_count)\n",
        "    \n",
        "    return np.split(std_hrvs, person_hrvs_count), scaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU626-ciV1zI"
      },
      "source": [
        "# Create HRVs array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNyeFwNtxnBu",
        "outputId": "e92d6606-146b-4569-fb5f-ff28be5f170e"
      },
      "outputs": [],
      "source": [
        "dataset = []\n",
        "\n",
        "for filename in tqdm(filenames):\n",
        "    annotation = wfdb.rdann(filename,\"atr\")\n",
        "    fs = annotation.fs\n",
        "    interv = slice_record_to_5min_intervals(filename)\n",
        "    hrvs = [] \n",
        "\n",
        "    for five_min in interv:\n",
        "        rpeaks = wave_to_peaks(five_min, fs)\n",
        "        rri = np.diff(rpeaks)\n",
        "        hrv = rpeaks_to_hrv(rri, fs)\n",
        "        hrvs.append(hrv)\n",
        "    \n",
        "    hrvs = [hrvs]\n",
        "    dataset.append(hrvs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3V1B1YeWWoo",
        "outputId": "7ed31b92-9a2a-4d2c-d4f5-dd0bcef946e7"
      },
      "outputs": [],
      "source": [
        "hrv_by_person = flatten_1stlevel(dataset)\n",
        "normalized_hrv = standardize_hrv_values(hrv_by_person)\n",
        "normalized_arr = np.array(normalized_hrv)\n",
        "print(normalized_arr.shape, '\\n', normalized_arr[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8hMyiaAlz3O"
      },
      "source": [
        "# Save array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIoj8dxW2Fnc"
      },
      "outputs": [],
      "source": [
        "np.save('', normalized_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyVEgTfQh0cD"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2TOgfF4Ekpw"
      },
      "source": [
        "Testing of \"slice_signal_to_5min_intervals\" func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IBkbTjeEpa7"
      },
      "outputs": [],
      "source": [
        "dummy_signal = [i for i in range(0, 100000)]\n",
        "dummy_fs = 3\n",
        "dummy_slices = slice_signal_to_5min_intervals(dummy_signal, dummy_fs)\n",
        "\n",
        "for s in dummy_slices:\n",
        "    print(s[:3],'...',s[-3:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-y7oHRn2uqr"
      },
      "source": [
        "Testing of \"create_dataset_by_person\" func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epADR9J82sQ9"
      },
      "outputs": [],
      "source": [
        "dummy_data = np.arange(0, 800).reshape(10, 8, 10)\n",
        "dummy_ds = create_dataset_by_person(dummy_data[0])\n",
        "\n",
        "for i in dummy_ds.as_numpy_iterator():\n",
        "    print(i)\n",
        "\n",
        "for x, y in create_dataset_by_person(test_arrs[0]):\n",
        "    print('x = ', x.numpy())\n",
        "    print('y = ', y.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KiWeVIUVWzP"
      },
      "source": [
        "Testing of \"standardize_hrv_values\" func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUb-2rqSGEGE"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "people = 5\n",
        "arrs_count = 16\n",
        "arr_length = 5\n",
        "sample_pop = range(100)\n",
        "test_arrs = []\n",
        "\n",
        "for ps in range(people):\n",
        "    this_person = []\n",
        "    \n",
        "    for ar in range(arrs_count):\n",
        "        this_person.append(random.sample(sample_pop, arr_length))\n",
        "    test_arrs.append(this_person)\n",
        "  \n",
        "print(test_arrs[0])\n",
        "std_dummy_data = standardize_hrv_values(test_arrs)\n",
        "print(std_dummy_data[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wJjxS7UpB2j4",
        "dEhFNgBRCag7",
        "fyVEgTfQh0cD"
      ],
      "name": "DatasetGenerator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
