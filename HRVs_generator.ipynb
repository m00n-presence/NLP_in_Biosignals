{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DatasetGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sTQk_2uX_3pY",
        "GdRUbIys-nNf",
        "wJjxS7UpB2j4",
        "RU626-ciV1zI",
        "gNoIOKko7ZRe",
        "Z_7IBq0r74rk",
        "t8hMyiaAlz3O",
        "fyVEgTfQh0cD"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTQk_2uX_3pY"
      },
      "source": [
        "# Import\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNJ56a4rzFAS"
      },
      "source": [
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJd8TqCuATLz"
      },
      "source": [
        "!pip install wfdb tqdm hrv-analysis\n",
        "!pip install -Iv neurokit2==0.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPCXQRlt_0wN"
      },
      "source": [
        "import neurokit2 as nk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import hrvanalysis as hrvana\n",
        "import wfdb\n",
        "import keras\n",
        "import scipy\n",
        "import os\n",
        "import math\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from wfdb import plot\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHSq1eqGTruL"
      },
      "source": [
        "Mount GDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcALmSBpE4U8",
        "outputId": "5ff5a402-871f-4d94-c42d-9de2ee3f6ee2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywTXL6r_zP0_"
      },
      "source": [
        "List of filenames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNtnNsMMxlkW"
      },
      "source": [
        "datafolder = r\"/content/drive/MyDrive/dbs/ltafdb/\"\n",
        "files = os.listdir(datafolder)\n",
        "filenames = list(set([datafolder + f.split('.')[0] for f in files]))\n",
        "filenames.sort()\n",
        "filenames.remove(datafolder + '30')\n",
        "filenames.remove(datafolder + '45')\n",
        "filenames.remove(datafolder + '75') # оставим 1 запись для тестирования модели"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVSxxFolo7me"
      },
      "source": [
        "filenames[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "GdRUbIys-nNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXCLUDED_COLUMNS = ['HRV_ULF', 'HRV_VLF', 'HRV_CSI_Modified', 'HRV_S'] # exclude 4 signs\n",
        "INCLUDED_COLUMNS = ['HRV_pNN50', 'HRV_CVI', 'HRV_HTI', 'HRV_CVNN', 'HRV_ApEn', 'HRV_SampEn', \n",
        "                    'HRV_SD1SD2', 'HRV_LFHF', 'HRV_IALS', 'HRV_PAS', 'HRV_PI', 'HRV_AI'] # get 12 signs in total"
      ],
      "metadata": {
        "id": "dDyIJNEs_ezP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJjxS7UpB2j4"
      },
      "source": [
        "Funcs for slicing record into 5 min length samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61C3nG2XB1RU"
      },
      "source": [
        "def slice_record_to_5min_intervals(filename, intervals_max_count = 180):\n",
        "    \"\"\" Читает запись и делит её на последовательные пятиминутные отрезки,\n",
        "        их максимальное количество = intervals_max_count\n",
        "        Args:\n",
        "            filename: string, полный путь до записи без расширения\n",
        "            intervals_max_count: int, максимальное количество интервалов, \n",
        "                на которые делить запись. Если она короче - вернет меньше интервалов\n",
        "        Returns:\n",
        "            list of lists: внутренние списки - списки сэмплов, входящих в интервалы\n",
        "    \"\"\"\n",
        "    print(\"Now slicing record with path : \" + filename)\n",
        "    annotation = wfdb.rdann(filename,\"atr\")\n",
        "    fs = annotation.fs\n",
        "    sample = annotation.sample\n",
        "    record = wfdb.rdrecord(filename)    \n",
        "    signal = record.__dict__[\"p_signal\"][sample[0]:sample[-1], 0]\n",
        "    \n",
        "    # return slice_signal_to_5min_intervals(signal, fs)\n",
        "    return slice_intersected_5min_intervals(signal, fs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbzHpjfdFnYA"
      },
      "source": [
        "def slice_signal_to_5min_intervals(signal, sampling_freq, intervals_max_count = 180):\n",
        "    \"\"\" Делит сигнал на последовательные пятиминутные отрезки,\n",
        "        их максимальное количество = intervals_max_count\n",
        "        Args: \n",
        "            signal: list-like, сигнал для деления\n",
        "            sampling_freq: int, частота дискретизации(Гц)\n",
        "            intervals_max_count: int,  максимальное количество интервалов, \n",
        "                на которые делить запись. Если она короче - вернет меньше интервалов\n",
        "        Returns:\n",
        "            list of lists: внутренние списки - списки сэмплов, входящих в интервалы\n",
        "            максимальная длина внешнего списка = intervals_max_count\n",
        "    \"\"\"\n",
        "    interval_length = 5 * 60 * sampling_freq\n",
        "    signal_slices = []\n",
        "    \n",
        "    for i in range(0, len(signal), interval_length):\n",
        "        if (len(signal_slices) >= intervals_max_count):\n",
        "            break\n",
        "        signal_slices.append(signal[i:i + interval_length]) \n",
        "\n",
        "    return signal_slices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7fiz3PUFoMm"
      },
      "source": [
        "def slice_intersected_5min_intervals(signal, sampling_freq, intervals_max_count = 250, shift_minutes = 1):\n",
        "    \"\"\" Делит сигнал на пересекающиеся 5-минутные отрезки \n",
        "        Args: \n",
        "            signal: list-like, сигнал\n",
        "            sampling_freq: int, частота дискретизации(Гц)\n",
        "            intervals_max_count: int, максимальное количество интервалов на возвращение,\n",
        "                может вернуть меньше указанного значения\n",
        "            shift_minutes: int > 0, шаг, примеры:\n",
        "                если 1, то возвратит интервалы 0-5, 1-6, 2-7 и т.д.\n",
        "                если 2, то интервалы 0-5, 2-7, 4-9 и т.д.\n",
        "        Returns:\n",
        "            list of lists - внутренние списки - сэмплы на 5 минут,\n",
        "            максимальное len() внешнего списка = intervals_max_count\n",
        "    \"\"\"\n",
        "    interval_length = 5 * 60 * sampling_freq\n",
        "    shift = shift_minutes * 60 * sampling_freq\n",
        "    signal_slices = []\n",
        "    \n",
        "    for i in range(0, len(signal), shift):\n",
        "        if (len(signal_slices) >= intervals_max_count):\n",
        "            break\n",
        "        current_slice = signal[i:i + interval_length]\n",
        "        signal_slices.append(current_slice)\n",
        "\n",
        "    return signal_slices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEhFNgBRCag7"
      },
      "source": [
        "Funcs for getting peaks and HRV array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9wAu5MLCdmd"
      },
      "source": [
        "# https://stackoverflow.com/questions/6518811/interpolate-nan-values-in-a-numpy-array\n",
        "def interpolate_nans(original):\n",
        "    \"\"\" Линейно интерполирует NaN и бесконечные значения в коллекции\n",
        "        Args:\n",
        "            original: array-like, коллекция, в кот-ой нужно интерполировать значения\n",
        "        Returns:\n",
        "            тот же объект, но все NaN и бесконечные значения заменены на \n",
        "            линейно интерполированные значения\n",
        "    \"\"\"\n",
        "    nans, x = ~np.isfinite(original), lambda z: z.nonzero()[0]\n",
        "    original[nans]= np.interp(x(nans), x(~nans), original[~nans])\n",
        "    \n",
        "    return original"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL6HHBgmQiSZ"
      },
      "source": [
        "def wave_to_peaks(signal, fs):\n",
        "    \"\"\" Декоратор (Wrapper function) neurokit's 'ecg_findpeaks' и 'ecg_clean'.\n",
        "        Очищает сигнал и возвращает сэмплы, на которых найдены R-пики,\n",
        "        как в документации neurokit по функциям.\n",
        "        Args: \n",
        "            signal: list-like, сигнал\n",
        "            fs: int, частота дискретизации(Гц)\n",
        "        Returns:\n",
        "            dict, доступ к пикам по ключу 'ECG_R_Peaks'\n",
        "    \"\"\"\n",
        "    signal = hrvana.interpolate_nan_values(rr_intervals=signal,interpolation_method=\"linear\")\n",
        "    cleaned = nk.ecg_clean(signal, sampling_rate = fs)\n",
        "    peaks = nk.ecg_findpeaks(cleaned, sampling_rate=fs, show=False)['ECG_R_Peaks']\n",
        "    \n",
        "    return peaks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgm6N_lNFRN_"
      },
      "source": [
        "def rpeaks_to_hrv(rri, fs, hrv_indices):\n",
        "    \"\"\" Возвращает HRV значения из R-пиков\n",
        "        Args:\n",
        "            rri: dict, доступ к пикам по ключу 'ECG_R_Peaks'\n",
        "            fs: int, частота дискретизации (Гц)\n",
        "        Returns:\n",
        "            1-мерный массив показателей\n",
        "    \"\"\"\n",
        "    clean_rri = rri*(1000/fs)\n",
        "    clean_rri = hrvana.remove_ectopic_beats(rr_intervals=clean_rri, method=\"malik\")\n",
        "    clean_rri = hrvana.interpolate_nan_values(rr_intervals=clean_rri,interpolation_method=\"linear\")\n",
        "\n",
        "    clean_rri = np.array(clean_rri)\n",
        "    clean_rri = clean_rri[~np.isnan(clean_rri)]\n",
        "    \n",
        "    peaks_unec = np.zeros(len(clean_rri)+1)\n",
        "    cv = 0\n",
        "\n",
        "    for count, value in enumerate(clean_rri):\n",
        "        cv += value\n",
        "        peaks_unec[count+1] = cv\n",
        "\n",
        "    peaks_unec *= (128.0/1000.0)   \n",
        "    rpeaks = {'ECG_R_Peaks':peaks_unec}\n",
        "    hrvdat = nk.hrv(rpeaks, sampling_rate=128, show=False)\n",
        "    hrvdat = hrvdat.drop(columns = hrv_indices)\n",
        "    \n",
        "    return np.ravel(hrvdat.to_numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_specified_hrv(rri, fs, hrv_indices):\n",
        "    \"\"\" Возвращает указанные показатели HRV\n",
        "        Args:\n",
        "            rri: dict, доступ к пикам по ключу 'ECG_R_Peaks'\n",
        "            fs: int, частота дискретизации (Гц)\n",
        "            hrv_indices: list, список HRV-показателей\n",
        "        Returns:\n",
        "            1-мерный numpy массив значений указанных показателей\n",
        "    \"\"\"\n",
        "    clean_rri = rri*(1000/fs)\n",
        "    clean_rri = hrvana.remove_ectopic_beats(rr_intervals=clean_rri, method=\"malik\")\n",
        "    clean_rri = hrvana.interpolate_nan_values(rr_intervals=clean_rri,interpolation_method=\"linear\")\n",
        "\n",
        "    clean_rri = np.array(clean_rri)\n",
        "    clean_rri = clean_rri[~np.isnan(clean_rri)]\n",
        "    \n",
        "    peaks_unec = np.zeros(len(clean_rri)+1)\n",
        "    cv = 0\n",
        "\n",
        "    for count, value in enumerate(clean_rri):\n",
        "        cv += value\n",
        "        peaks_unec[count+1] = cv\n",
        "\n",
        "    peaks_unec *= (128.0/1000.0)   \n",
        "    rpeaks = {'ECG_R_Peaks':peaks_unec}\n",
        "    hrv_full = nk.hrv(rpeaks, sampling_rate=128, show=False)\n",
        "    target_hrvs = hrv_full[hrv_indices]\n",
        "    \n",
        "    return np.ravel(target_hrvs.to_numpy())"
      ],
      "metadata": {
        "id": "PLEJN1sZ9CLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69P8c3rtSJPV"
      },
      "source": [
        "Funcs for normalizing the HRV values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-ZVMGnQCjeG"
      },
      "source": [
        "def flatten_1stlevel(list_of_lists_of_lists):\n",
        "    \"\"\" >>> flatten_1stlevel([[[1, 2], [3, 4]],[[5, 6], [7, 8]],[[9, 0]]])\n",
        "        [[1, 2], [3, 4], [5, 6], [7, 8], [9, 0]]\n",
        "        >>> flatten_1stlevel([[1, 2], [[3,4], [[5, 6], 7]], []])\n",
        "        [1, 2, [3, 4], [[5, 6], 7]]\n",
        "        >>> flatten_1stlevel([[1, 2], [3, 4]])\n",
        "        [1, 2, 3, 4]\n",
        "        >>> flatten_1stlevel([1, 2, 3])\n",
        "        [1, 2, 3]\n",
        "    \"\"\"\n",
        "    flat_1stlevel = []\n",
        "    \n",
        "    for list_of_lists in list_of_lists_of_lists:\n",
        "        try:\n",
        "            flat_1stlevel.extend(list_of_lists)\n",
        "        except TypeError:\n",
        "            flat_1stlevel.append(list_of_lists)\n",
        "\n",
        "    return flat_1stlevel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSLJJkzFOI5"
      },
      "source": [
        "def standardize_hrv_values(hrv_arrays, return_scaler=False):\n",
        "    \"\"\" Нормализует значения показателей с помощью MinMaxScaler\n",
        "        Args:\n",
        "            hrv_arrays: list of arrays, список массивов показателей по пациенту, структура:\n",
        "            [[[1, 2], [3, 4]],[[5, 6], [7, 8]],[[9, 0], [10, 11]]]\n",
        "            return_scaler: bool, true - возвращает обученный MinMaxScaler\n",
        "        Returns:\n",
        "            (всегда) list of numpy arrays - стандартизированных значений,\n",
        "            if return_scaler - также возвратит MinMaxScaler\n",
        "\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    person_hrvs_count = len(hrv_arrays) # количество пациентов\n",
        "    all_hrvs = flatten_1stlevel(hrv_arrays)\n",
        "    scaler = scaler.fit(all_hrvs)\n",
        "    std_hrvs = scaler.transform(all_hrvs)\n",
        "    \n",
        "    if not return_scaler:\n",
        "        return np.split(std_hrvs, person_hrvs_count)\n",
        "    \n",
        "    return np.split(std_hrvs, person_hrvs_count), scaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU626-ciV1zI"
      },
      "source": [
        "# Create a dataset without EXCLUDED_COLUMNS (in total: 48 signs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNyeFwNtxnBu"
      },
      "source": [
        "dataset = []\n",
        "\n",
        "for filename in tqdm(filenames):\n",
        "    annotation = wfdb.rdann(filename,\"atr\")\n",
        "    fs = annotation.fs\n",
        "    interv = slice_record_to_5min_intervals(filename)\n",
        "    hrvs = [] \n",
        "\n",
        "    for five_min in interv:\n",
        "        rpeaks = wave_to_peaks(five_min, fs)\n",
        "        rri = np.diff(rpeaks)\n",
        "        hrv = rpeaks_to_hrv(rri, fs, EXCLUDED_COLUMNS)\n",
        "        hrvs.append(hrv)\n",
        "    \n",
        "    hrvs = [hrvs]\n",
        "    dataset.append(hrvs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a dataset with INCLUDED_COLUMNS (in total: 12 signs)"
      ],
      "metadata": {
        "id": "gNoIOKko7ZRe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvm7WWgg_RZN"
      },
      "source": [
        "dataset = []\n",
        "\n",
        "for filename in tqdm(filenames):\n",
        "    annotation = wfdb.rdann(filename,\"atr\")\n",
        "    fs = annotation.fs\n",
        "    interv = slice_record_to_5min_intervals(filename)\n",
        "    hrvs = [] \n",
        "\n",
        "    for five_min in interv:\n",
        "        rpeaks = wave_to_peaks(five_min, fs)\n",
        "        rri = np.diff(rpeaks)\n",
        "        hrv = get_specified_hrv(rri, fs, INCLUDED_COLUMNS)\n",
        "        hrvs.append(hrv)\n",
        "    \n",
        "    hrvs = [hrvs]\n",
        "    dataset.append(hrvs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize the HRV values"
      ],
      "metadata": {
        "id": "Z_7IBq0r74rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hrv_by_person = flatten_1stlevel(dataset)\n",
        "normalized_hrv = standardize_hrv_values(hrv_by_person)\n",
        "normalized_arr = np.array(normalized_hrv)\n",
        "print(normalized_arr.shape, '\\n', normalized_arr[0])"
      ],
      "metadata": {
        "id": "MqvDbiXa74YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8hMyiaAlz3O"
      },
      "source": [
        "# Save a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIoj8dxW2Fnc"
      },
      "source": [
        "np.save('', normalized_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyVEgTfQh0cD"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2TOgfF4Ekpw"
      },
      "source": [
        "Testing of \"slice_signal_to_5min_intervals\" func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IBkbTjeEpa7"
      },
      "source": [
        "dummy_signal = [i for i in range(0, 100000)]\n",
        "dummy_fs = 3\n",
        "dummy_slices = slice_signal_to_5min_intervals(dummy_signal, dummy_fs)\n",
        "\n",
        "for s in dummy_slices:\n",
        "    print(s[:3],'...',s[-3:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-y7oHRn2uqr"
      },
      "source": [
        "Testing of \"create_dataset_by_person\" func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epADR9J82sQ9"
      },
      "source": [
        "dummy_data = np.arange(0, 800).reshape(10, 8, 10)\n",
        "dummy_ds = create_dataset_by_person(dummy_data[0])\n",
        "\n",
        "for i in dummy_ds.as_numpy_iterator():\n",
        "    print(i)\n",
        "\n",
        "for x, y in create_dataset_by_person(test_arrs[0]):\n",
        "    print('x = ', x.numpy())\n",
        "    print('y = ', y.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KiWeVIUVWzP"
      },
      "source": [
        "Testing of \"standardize_hrv_values\" func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUb-2rqSGEGE"
      },
      "source": [
        "import random\n",
        "\n",
        "\n",
        "people = 5\n",
        "arrs_count = 16\n",
        "arr_length = 5\n",
        "sample_pop = range(100)\n",
        "test_arrs = []\n",
        "\n",
        "for ps in range(people):\n",
        "    this_person = []\n",
        "    \n",
        "    for ar in range(arrs_count):\n",
        "        this_person.append(random.sample(sample_pop, arr_length))\n",
        "    test_arrs.append(this_person)\n",
        "  \n",
        "print(test_arrs[0])\n",
        "std_dummy_data = standardize_hrv_values(test_arrs)\n",
        "print(std_dummy_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}